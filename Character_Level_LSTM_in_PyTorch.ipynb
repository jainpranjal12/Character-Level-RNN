{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character-Level LSTM in PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yQ1ywkt6etn2",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "aebcb6e6-6e5f-4d64-b84a-5171f1130a2e"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9c95f5ac-2936-4000-bb53-e568e4f570bc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9c95f5ac-2936-4000-bb53-e568e4f570bc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving anna.txt to anna.txt\n",
            "User uploaded file \"anna.txt\" with length 2025486 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wZy9knGzetoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1APNIpAzetoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c81122ea-6d08-4bad-b58f-d5858ed5411d"
      },
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced to\\nher husband that she could not go on living in the same house with him.\\nThis position of affairs had now lasted three days, and not only the\\nhusband and wife themselves, but all the members of their family and\\nhousehold, were painfully conscious of it. Every person in the house\\nfelt that there was no sense in their living together, and that the\\nstray people brought together by chance in any inn had more in common\\nwith one another than they, the members of the family and household of\\nthe Oblonskys. The wife did not leave her own room, the husband had not\\nbeen at home for three days. The children ran wild all over the house;\\nthe English governess quarreled with the housekeeper, and wrote to a\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "rEpDrWuHetoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsI7Rjw0etol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "77b164b2-4705-4aea-e5f5-7bb09982c1c9"
      },
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37, 28,  5, 44, 51, 66, 77,  0, 13, 79, 79, 79, 25,  5, 44, 44, 43,\n",
              "        0, 78,  5, 75,  3, 19,  3, 66, 12,  0,  5, 77, 66,  0,  5, 19, 19,\n",
              "        0,  5, 19,  3, 45, 66, 15,  0, 66, 39, 66, 77, 43,  0, 26, 41, 28,\n",
              "        5, 44, 44, 43,  0, 78,  5, 75,  3, 19, 43,  0,  3, 12,  0, 26, 41,\n",
              "       28,  5, 44, 44, 43,  0,  3, 41,  0,  3, 51, 12,  0, 67, 31, 41, 79,\n",
              "       31,  5, 43,  2, 79, 79, 54, 39, 66, 77, 43, 51, 28,  3, 41])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "DmSQ3eiyetox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-XiIp72deto6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "92c5968f-334d-45ed-b5f0-7130e2781179"
      },
      "cell_type": "code",
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NhSlxl1getpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "soVHBFZYetpO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSKFdcOLetpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "3a662ba8-2e61-4ed3-acc6-245f37f92363"
      },
      "cell_type": "code",
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[37 28  5 44 51 66 77  0 13 79]\n",
            " [12 67 41  0 51 28  5 51  0  5]\n",
            " [66 41 70  0 67 77  0  5  0 78]\n",
            " [12  0 51 28 66  0 46 28  3 66]\n",
            " [ 0 12  5 31  0 28 66 77  0 51]\n",
            " [46 26 12 12  3 67 41  0  5 41]\n",
            " [ 0 23 41 41  5  0 28  5 70  0]\n",
            " [81 52 19 67 41 12 45 43  2  0]]\n",
            "\n",
            "y\n",
            " [[28  5 44 51 66 77  0 13 79 79]\n",
            " [67 41  0 51 28  5 51  0  5 51]\n",
            " [41 70  0 67 77  0  5  0 78 67]\n",
            " [ 0 51 28 66  0 46 28  3 66 78]\n",
            " [12  5 31  0 28 66 77  0 51 66]\n",
            " [26 12 12  3 67 41  0  5 41 70]\n",
            " [23 41 41  5  0 28  5 70  0 12]\n",
            " [52 19 67 41 12 45 43  2  0 65]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cU0jAtzxetpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7ccd052-0a68-45a5-dc55-97de9f2c7c86"
      },
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r5WBRpjQetp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpvICAbhetqP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XOKzjGWKetqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7d69b0fb-9ced-416d-af8c-2f8a2a08018e"
      },
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bd_iJ-tNetql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4743
        },
        "outputId": "762e9c70-772b-4f1b-b9cd-21bcd3b0e3fc"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2522... Val Loss: 3.1826\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1348... Val Loss: 3.1265\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1389... Val Loss: 3.1207\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1176... Val Loss: 3.1195\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1444... Val Loss: 3.1172\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1141... Val Loss: 3.1151\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1054... Val Loss: 3.1130\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1177... Val Loss: 3.1060\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1047... Val Loss: 3.0898\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0679... Val Loss: 3.0531\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0024... Val Loss: 2.9904\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8686... Val Loss: 2.8643\n",
            "Epoch: 1/20... Step: 130... Loss: 2.8158... Val Loss: 2.7838\n",
            "Epoch: 2/20... Step: 140... Loss: 2.7549... Val Loss: 2.6752\n",
            "Epoch: 2/20... Step: 150... Loss: 2.6323... Val Loss: 2.5798\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5429... Val Loss: 2.5081\n",
            "Epoch: 2/20... Step: 170... Loss: 2.4800... Val Loss: 2.4608\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4521... Val Loss: 2.4239\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4024... Val Loss: 2.3877\n",
            "Epoch: 2/20... Step: 200... Loss: 2.3868... Val Loss: 2.3778\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3585... Val Loss: 2.3314\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3173... Val Loss: 2.2987\n",
            "Epoch: 2/20... Step: 230... Loss: 2.2952... Val Loss: 2.2653\n",
            "Epoch: 2/20... Step: 240... Loss: 2.2773... Val Loss: 2.2376\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2138... Val Loss: 2.2128\n",
            "Epoch: 2/20... Step: 260... Loss: 2.1839... Val Loss: 2.1885\n",
            "Epoch: 2/20... Step: 270... Loss: 2.1940... Val Loss: 2.1674\n",
            "Epoch: 3/20... Step: 280... Loss: 2.1861... Val Loss: 2.1378\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1518... Val Loss: 2.1151\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1199... Val Loss: 2.0972\n",
            "Epoch: 3/20... Step: 310... Loss: 2.0973... Val Loss: 2.0769\n",
            "Epoch: 3/20... Step: 320... Loss: 2.0696... Val Loss: 2.0547\n",
            "Epoch: 3/20... Step: 330... Loss: 2.0332... Val Loss: 2.0362\n",
            "Epoch: 3/20... Step: 340... Loss: 2.0651... Val Loss: 2.0225\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0336... Val Loss: 2.0043\n",
            "Epoch: 3/20... Step: 360... Loss: 1.9733... Val Loss: 1.9859\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0062... Val Loss: 1.9731\n",
            "Epoch: 3/20... Step: 380... Loss: 1.9804... Val Loss: 1.9565\n",
            "Epoch: 3/20... Step: 390... Loss: 1.9627... Val Loss: 1.9393\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9286... Val Loss: 1.9235\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9336... Val Loss: 1.9076\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9282... Val Loss: 1.8952\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9198... Val Loss: 1.8779\n",
            "Epoch: 4/20... Step: 440... Loss: 1.9024... Val Loss: 1.8683\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8369... Val Loss: 1.8532\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8376... Val Loss: 1.8463\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8626... Val Loss: 1.8319\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8405... Val Loss: 1.8162\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8416... Val Loss: 1.8027\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8332... Val Loss: 1.7938\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8167... Val Loss: 1.7807\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8300... Val Loss: 1.7712\n",
            "Epoch: 4/20... Step: 530... Loss: 1.7962... Val Loss: 1.7636\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7483... Val Loss: 1.7510\n",
            "Epoch: 4/20... Step: 550... Loss: 1.8001... Val Loss: 1.7395\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7615... Val Loss: 1.7349\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7485... Val Loss: 1.7238\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7174... Val Loss: 1.7121\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7279... Val Loss: 1.7042\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7184... Val Loss: 1.6962\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7041... Val Loss: 1.6913\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7090... Val Loss: 1.6835\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7243... Val Loss: 1.6771\n",
            "Epoch: 5/20... Step: 640... Loss: 1.6848... Val Loss: 1.6705\n",
            "Epoch: 5/20... Step: 650... Loss: 1.6716... Val Loss: 1.6600\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6549... Val Loss: 1.6552\n",
            "Epoch: 5/20... Step: 670... Loss: 1.6791... Val Loss: 1.6486\n",
            "Epoch: 5/20... Step: 680... Loss: 1.6758... Val Loss: 1.6424\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6558... Val Loss: 1.6370\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6579... Val Loss: 1.6300\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6385... Val Loss: 1.6231\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6304... Val Loss: 1.6149\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6436... Val Loss: 1.6088\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6100... Val Loss: 1.6051\n",
            "Epoch: 6/20... Step: 750... Loss: 1.5873... Val Loss: 1.6003\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6393... Val Loss: 1.5976\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6058... Val Loss: 1.5935\n",
            "Epoch: 6/20... Step: 780... Loss: 1.5979... Val Loss: 1.5884\n",
            "Epoch: 6/20... Step: 790... Loss: 1.5795... Val Loss: 1.5816\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6063... Val Loss: 1.5740\n",
            "Epoch: 6/20... Step: 810... Loss: 1.5830... Val Loss: 1.5716\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5543... Val Loss: 1.5669\n",
            "Epoch: 6/20... Step: 830... Loss: 1.5989... Val Loss: 1.5611\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5423... Val Loss: 1.5585\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5690... Val Loss: 1.5545\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5465... Val Loss: 1.5482\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5581... Val Loss: 1.5441\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5484... Val Loss: 1.5409\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5524... Val Loss: 1.5362\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5293... Val Loss: 1.5300\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5097... Val Loss: 1.5285\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5332... Val Loss: 1.5280\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5164... Val Loss: 1.5236\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5237... Val Loss: 1.5208\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5313... Val Loss: 1.5148\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5348... Val Loss: 1.5108\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5326... Val Loss: 1.5073\n",
            "Epoch: 8/20... Step: 980... Loss: 1.5044... Val Loss: 1.5092\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5090... Val Loss: 1.5014\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.4987... Val Loss: 1.4953\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5410... Val Loss: 1.4901\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5088... Val Loss: 1.4900\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.4939... Val Loss: 1.4857\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5029... Val Loss: 1.4870\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.4783... Val Loss: 1.4845\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.4736... Val Loss: 1.4839\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.4910... Val Loss: 1.4761\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.4823... Val Loss: 1.4742\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4670... Val Loss: 1.4711\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4551... Val Loss: 1.4697\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4630... Val Loss: 1.4629\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.4764... Val Loss: 1.4628\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.4677... Val Loss: 1.4616\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.4735... Val Loss: 1.4550\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.4880... Val Loss: 1.4529\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4501... Val Loss: 1.4534\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4555... Val Loss: 1.4516\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4394... Val Loss: 1.4512\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.4786... Val Loss: 1.4474\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4205... Val Loss: 1.4453\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4375... Val Loss: 1.4411\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4410... Val Loss: 1.4403\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4149... Val Loss: 1.4364\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4217... Val Loss: 1.4340\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4296... Val Loss: 1.4325\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4328... Val Loss: 1.4330\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4258... Val Loss: 1.4312\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4352... Val Loss: 1.4257\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4273... Val Loss: 1.4248\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4115... Val Loss: 1.4236\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4273... Val Loss: 1.4223\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.3853... Val Loss: 1.4204\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4053... Val Loss: 1.4193\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.3938... Val Loss: 1.4158\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.3767... Val Loss: 1.4127\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.3871... Val Loss: 1.4112\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.3782... Val Loss: 1.4125\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4205... Val Loss: 1.4053\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4234... Val Loss: 1.4045\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4224... Val Loss: 1.4034\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4431... Val Loss: 1.4043\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4255... Val Loss: 1.3990\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.3903... Val Loss: 1.4025\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4184... Val Loss: 1.3961\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3464... Val Loss: 1.3982\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3714... Val Loss: 1.3941\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3640... Val Loss: 1.3940\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3799... Val Loss: 1.3897\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3819... Val Loss: 1.3872\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3630... Val Loss: 1.3906\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3408... Val Loss: 1.3859\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.3750... Val Loss: 1.3822\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4293... Val Loss: 1.3813\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3820... Val Loss: 1.3809\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.3901... Val Loss: 1.3812\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4053... Val Loss: 1.3755\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3440... Val Loss: 1.3776\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3227... Val Loss: 1.3756\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3223... Val Loss: 1.3731\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3438... Val Loss: 1.3732\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3423... Val Loss: 1.3740\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3435... Val Loss: 1.3650\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3637... Val Loss: 1.3640\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3336... Val Loss: 1.3635\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3066... Val Loss: 1.3583\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3653... Val Loss: 1.3568\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3375... Val Loss: 1.3583\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.3536... Val Loss: 1.3554\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3225... Val Loss: 1.3513\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3228... Val Loss: 1.3502\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.2934... Val Loss: 1.3474\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3105... Val Loss: 1.3444\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.3467... Val Loss: 1.3460\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3093... Val Loss: 1.3495\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.2873... Val Loss: 1.3499\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3129... Val Loss: 1.3434\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3341... Val Loss: 1.3449\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3017... Val Loss: 1.3390\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.2907... Val Loss: 1.3337\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3140... Val Loss: 1.3342\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3163... Val Loss: 1.3371\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3038... Val Loss: 1.3318\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3207... Val Loss: 1.3307\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2710... Val Loss: 1.3352\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2598... Val Loss: 1.3312\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3190... Val Loss: 1.3285\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3095... Val Loss: 1.3249\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3098... Val Loss: 1.3273\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3265... Val Loss: 1.3323\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.2984... Val Loss: 1.3249\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.2969... Val Loss: 1.3243\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.2954... Val Loss: 1.3238\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2650... Val Loss: 1.3230\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3188... Val Loss: 1.3218\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.2807... Val Loss: 1.3338\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.2992... Val Loss: 1.3179\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.2775... Val Loss: 1.3169\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.2784... Val Loss: 1.3211\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2763... Val Loss: 1.3306\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2701... Val Loss: 1.3173\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.2827... Val Loss: 1.3146\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3020... Val Loss: 1.3139\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2707... Val Loss: 1.3129\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.2795... Val Loss: 1.3118\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2619... Val Loss: 1.3151\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.2854... Val Loss: 1.3122\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.2722... Val Loss: 1.3103\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2827... Val Loss: 1.3073\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.2851... Val Loss: 1.3119\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2704... Val Loss: 1.3076\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2672... Val Loss: 1.3041\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.2668... Val Loss: 1.3101\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2416... Val Loss: 1.3097\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2453... Val Loss: 1.3060\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.2769... Val Loss: 1.3045\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2533... Val Loss: 1.3039\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2527... Val Loss: 1.3089\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2492... Val Loss: 1.3029\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.2618... Val Loss: 1.3037\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2459... Val Loss: 1.3033\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2148... Val Loss: 1.2981\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2724... Val Loss: 1.3014\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2385... Val Loss: 1.3047\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2503... Val Loss: 1.2967\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2269... Val Loss: 1.2958\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2443... Val Loss: 1.3032\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2492... Val Loss: 1.2986\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2566... Val Loss: 1.2933\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2520... Val Loss: 1.2936\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2162... Val Loss: 1.2950\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2339... Val Loss: 1.2998\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2329... Val Loss: 1.2944\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2345... Val Loss: 1.2921\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2486... Val Loss: 1.2950\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2504... Val Loss: 1.2901\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2508... Val Loss: 1.2921\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2374... Val Loss: 1.2932\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2321... Val Loss: 1.2880\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2340... Val Loss: 1.2903\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2574... Val Loss: 1.2920\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2485... Val Loss: 1.2890\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2315... Val Loss: 1.2863\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2449... Val Loss: 1.2883\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2203... Val Loss: 1.2887\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2140... Val Loss: 1.2882\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2311... Val Loss: 1.2887\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2292... Val Loss: 1.2846\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2210... Val Loss: 1.2852\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2126... Val Loss: 1.2857\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2166... Val Loss: 1.2888\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2272... Val Loss: 1.2842\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2361... Val Loss: 1.2823\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2400... Val Loss: 1.2850\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2546... Val Loss: 1.2828\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2051... Val Loss: 1.2865\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2297... Val Loss: 1.2786\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2118... Val Loss: 1.2803\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2551... Val Loss: 1.2799\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2017... Val Loss: 1.2807\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2034... Val Loss: 1.2823\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2186... Val Loss: 1.2810\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.1961... Val Loss: 1.2813\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2050... Val Loss: 1.2777\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2273... Val Loss: 1.2767\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2090... Val Loss: 1.2785\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2174... Val Loss: 1.2766\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2322... Val Loss: 1.2764\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2233... Val Loss: 1.2776\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2058... Val Loss: 1.2780\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2190... Val Loss: 1.2759\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.1901... Val Loss: 1.2750\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.1966... Val Loss: 1.2720\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.1894... Val Loss: 1.2751\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.1825... Val Loss: 1.2742\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.1846... Val Loss: 1.2732\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.1812... Val Loss: 1.2771\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2258... Val Loss: 1.2687\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2458... Val Loss: 1.2674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N0wP2rNQfE6D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ARV9mAYCetq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cc5qPVjXfDbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuYD_nZTetq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBK3xJdnetrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "527bd5b5-f7d9-4ad6-a045-06e30e3c85e3"
      },
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna had\n",
            "been both as they are said in the count to the carriage that he\n",
            "was a little things, and and the pretent of them he said that she was the\n",
            "little peasants, and when the ceasing trick of her easy were a letter and\n",
            "high and anything.\n",
            "\n",
            "Her listened to tee her foreveal, with her tone to ask, standing at his\n",
            "compretense of the same time.\n",
            "\n",
            "\n",
            "\n",
            "Chapter 22\n",
            "\n",
            "\n",
            "\"Alexay!\"\n",
            "\n",
            "\"Yes, but to say something what's not in my first. What were she in all\n",
            "the time. I'll set to them to go. I've taken in a letter in all-talking oving\n",
            "a percopplex of all...\"\n",
            "\n",
            "\"Not indeed, but it's an expensible, and some of the profoss. Bet the\n",
            "would be talking to the sorriege for, at a love and this soul, where is\n",
            "nothender and my sister--in--his bean, the country. When I do believe\n",
            "you want to take me to see you so mearing. To think of the chold worse,\n",
            "and I went on. I am so dang and a consideration will be the secret sense of\n",
            "an isten in the morning and what we see you and that it's a particurar foots and\n",
            "men, there are the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IhZjaxe9etrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5thNID6etrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "a25d0e11-2d3f-49ba-d18a-f4eec55f921a"
      },
      "cell_type": "code",
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Anna said\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Anna said\n",
            "that she was suffiried, and that the store had no money, went\n",
            "up to them.\n",
            "\n",
            "\"I've to be done.\"\n",
            "\n",
            "\"You must bot fail of the peeple. The presence is similital and married\n",
            "to see her.\"\n",
            "\n",
            "\"I say thought to be anyone all to have the round too. And it say, it\n",
            "is nothing,\" said the plant of hand, smiling at his house.\n",
            "\n",
            "Setcherivary horses are never concentrated it with silence,\n",
            "and still the cold. The particular and feeling of their called the\n",
            "sama saying which she was thought, the prince had a more one there. And showed\n",
            "her for her arrived, but she could not to take him in the face and he was\n",
            "trees, and soone had stome, and which was a string and sense of the same\n",
            "any familial complitice.\n",
            "\n",
            "\"What shall you say to stand?\" said the same, and said, as though it\n",
            "would not stand, who saw.\n",
            "\n",
            "\"That's the same someone,\" said the choir, and that his fresh clubbers\n",
            "of her hand and completely discussing; \"they were never still all\n",
            "the most capive to tell you her. And it seems, a springs or short will be\n",
            "the more any official traces and his weakness of seese. And how in him. And they\n",
            "dident almost all times, but he's to be a part for me to be man in a simpla\n",
            "single and the whole moment there was no trings on a prestions of the first\n",
            "tail.\n",
            "\n",
            "She's not the second mare. If the open feature, and with the standing from hands. They\n",
            "don't like the weakness and sort as in the matters of them of it. And the\n",
            "day. And it's so much a considerable familiar cold transs the men with\n",
            "the choic of the meadow to subject; but I went then to say he\n",
            "that he's been so serrably and still, the carriage, while you knew it's not\n",
            "all that simply but all three things,\" she said, with a storm way the proper\n",
            "trivasion of subject. \"It would be such seeing it and they's that hurry to say\n",
            "to this tomerrow, and have such tellow.\" As though she had been ball at\n",
            "the carriage of it one trouble of support of her friend.\"\n",
            "\n",
            "\"Oh, nothing stopen. Why should it's that if I suppose? What should you mind im...\"\n",
            "\n",
            "\"No, it's to ball, I \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q_80fyxwetrk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}